# Read the file generated by DeepSeek and apply interest clustering
import os
import time
import re
import jsonlines
import numpy as np
import pandas as pd
from tqdm import tqdm
from utils import clean_text
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer

version = 'v1'

# Choose the dataset to process.
dataset = 'book-crossing'
# dataset = 'dbbook2014'
# dataset = 'ml1m'

# Choose a word embedding method
cluster_type = 'fasttext'
# cluster_type = 'word2vec'

# Choose a clustering method
classify_method = 'hdbscan'

if dataset in ['dbbook2014', 'book-crossing']:
    field = 'books'
else:
    field = 'movies'

r = '_r'
C = 350
max_his_num = 30

data_path = f'./data/{dataset}'

output_path = f'batch_output/{dataset}_max{max_his_num}_output.jsonl'

if dataset in ['dbbook2014', 'ml1m']:
    item_list = pd.read_csv(data_path + '/item_list.txt', sep=' ')
    entity_list = pd.read_csv(data_path + '/entity_list.txt', sep=' ')
else:
    item_list = list()
    entity_list = list()
    lines = open(data_path + '/item_list.txt', 'r').readlines()

    for l in lines[1:]:
        if l == "\n":
            continue
        tmps = l.replace("\n", "").strip()
        elems = tmps.split(' ')

        org_id = elems[0]
        remap_id = elems[1]
        if len(elems[2:]) == 0:
            continue
        title = ' '.join(elems[2:])
        item_list.append([org_id, remap_id, title])
    item_list = pd.DataFrame(item_list, columns=['org_id', 'remap_id', 'entity_name'])

    lines = open(data_path + '/entity_list.txt', 'r').readlines()
    for l in lines[1:]:
        if l == "\n":
            continue
        tmps = l.replace("\n", "").strip()
        elems = tmps.split()
        org_id = elems[0]
        remap_id = elems[1]
        entity_list.append([org_id, remap_id])
    entity_list = pd.DataFrame(entity_list, columns=['org_id', 'remap_id'])

kg = pd.read_csv(data_path + '/kg_final.txt', sep=' ', names=['head', 'relation', 'tail'])
relation = pd.read_csv(data_path + '/relation_list.txt', sep=' ')
entity_list['remap_id'] = entity_list['remap_id'].astype(int)

relation_intent = kg['relation'].max() + 1
relation_sim = kg['relation'].max() + 2
items = item_list['remap_id'].to_list()

lower = 1
upper = 50
llm_answer = []
intent_triples = []
sim_triples = set()
intents_list = []
check_intent = []
waste = []

with open(output_path, mode='r') as f:
    for answer in jsonlines.Reader(f):
        row_id = answer['custom_id']

        raw_intents = answer['response']['body']['choices'][0]['message']['content']

        clean_intents = [clean_text(it) for it in raw_intents.strip().split(',')]

        intents = []
        for it in clean_intents:
            if r == '':
                if len(it.split()) > lower and len(it.split()) <= upper:
                    intents.append(it)
                else:
                    print(f'waste:', it.split())
                    waste.append(it)
            elif r == '_r':
                if len(it) > lower and len(it) <= upper:
                    intents.append(it)
                else:
                    print(f'waste:', it)
                    waste.append(it)

        for it in intents:
            intent_triples.append([row_id, it])
        intents_list.extend(intents)


def encode_intents(cluster_type, intents_list):

    if cluster_type == 'fasttext':
        from gensim.models import FastText
        import numpy as np
        # import nltk
        # nltk.download('wordnet')

        tokenized_data = [intent.lower().split() for intent in intents_list]

        model = FastText(
            sentences=tokenized_data,
            vector_size=100,
            window=5,
            min_count=1,
            sg=1,
            epochs=100
        )

        embeddings = []
        for tokens in tokenized_data:
            vec = np.mean([model.wv[token] for token in tokens if token in model.wv], axis=0)
            embeddings.append(vec)
        return np.array(embeddings)

    if cluster_type == 'word2vec':
        from gensim.models import Word2Vec
        import numpy as np
        from nltk.tokenize import word_tokenize
        import nltk
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', download_dir='./nltk_data')
            nltk.data.path.append('./nltk_data')
        tokenized_data = [word_tokenize(intent.lower()) for intent in intents_list]

        model = Word2Vec(
            sentences=tokenized_data,
            vector_size=128,
            window=3,
            min_count=1,
            sg=1,
            hs=1,
            epochs=200,
            workers=4
        )

        embeddings = []
        for tokens in tokenized_data:
            valid_tokens = [t for t in tokens if t in model.wv]
            if valid_tokens:
                vec = np.mean([model.wv[t] for t in valid_tokens], axis=0)
            else:
                vec = np.zeros(model.vector_size)
            embeddings.append(vec)
        return np.array(embeddings)

    if cluster_type == 'st':
        device = 'cuda:0'
        model = SentenceTransformer('./all-MiniLM-L6-v2')
        model.to(device)
        sentences = intents_list
        embeddings = model.encode(sentences, device=device)
    elif cluster_type == 'tfidf':
        sentences = intents_list
        vectorizer = TfidfVectorizer(max_df=0.8, ngram_range=(1, 2))
        embeddings = vectorizer.fit_transform(sentences)
    return embeddings

embeddings = encode_intents(cluster_type, intents_list)
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
print(f'cluster num: {C}')

cluster_ids = 0
if classify_method == 'kmeans':
    kmeans = KMeans(n_clusters=C, n_init=10)
    print("KMeans begin")
    print(embeddings.shape)
    cluster_ids = kmeans.fit_predict(embeddings)
elif classify_method == 'hdbscan':
    from hdbscan import HDBSCAN

    clusterer = HDBSCAN(min_cluster_size=5, min_samples=2, metric='euclidean',
                        cluster_selection_method='eom')
    print("hdbscan begin")
    print(embeddings.shape)

    cluster_ids = clusterer.fit_predict(embeddings)

print(cluster_ids)
print(cluster_ids.shape)
print("classify completed")

output_dict = dict(zip(intents_list, cluster_ids))

entity_add = []
entity_max = entity_list['remap_id'].max() + 1
for k, v in output_dict.items():
    entity_add.append([k, v + entity_max])
    output_dict[k] = v + entity_max
entity_add = pd.DataFrame(entity_add, columns=['org_id', 'remap_id'])

intent_triples_df = pd.DataFrame(intent_triples, columns=['uid', 'interest'])
intent_triples_df['merged_interest'] = intent_triples_df['interest'].map(output_dict)
user_num = len(intent_triples_df['uid'].unique())
print(f'user num:{user_num}')

intent_triples_df_group = intent_triples_df.groupby('merged_interest').count()
intent_triples_df_sort = intent_triples_df_group.sort_values(by='uid', ascending=False)

del_list = []
del_list.extend(list(intent_triples_df_sort[intent_triples_df_sort['uid'] >= int(user_num / 5)].index))
print(len(del_list))
del_list.extend(intent_triples_df_sort[intent_triples_df_sort['uid'] == 1].index)
intent_triples_df_filter = intent_triples_df[~intent_triples_df['merged_interest'].isin(del_list)]

print('interest graph edge num:', len(intent_triples_df_filter))
print('interest graph sparsity:', round(len(intent_triples_df_filter) / (user_num * C), 4) * 100, '%')

kg_intent = intent_triples_df_filter.loc[:, ['uid', 'merged_interest']]
kg_intent.columns = ['uid', 'interest']
kg_intent.to_csv(data_path + f'/kg_interest_C{C}_{cluster_type}_{version}{r}_{classify_method}_{dataset}.txt', sep=' ',
                 index=False, header=None)
